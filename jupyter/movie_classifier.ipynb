{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we try to predict movie genres based on the movie's poster and its overview.\n",
    "\n",
    "Posters for each film were obtained using the TMDB API and were saved as a normalized numpy array with each element of the array corresponding to a movie. The model used takes the poster as one input along with the overview as another input. The poster in input to a CNN while the overview is input to an LSTM. The output of each network is concatenated. The final fully connected layer consisted of 18 sigmoid output units. Each unit carries the probability of a movie belonging to a particular genre.\n",
    "\n",
    "we use Pytorch to implement classfication task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ming/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ming/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ming/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.style as style\n",
    "import ast\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from string import punctuation\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "style.use('seaborn-poster')\n",
    "style.use('ggplot')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading the image numpy arrays for the train,validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('content/movie_prediction/train_np_imgs_norm','rb') as f: X_img_train = pickle.load(f)\n",
    "with open('content/movie_prediction/test_np_imgs_norm', 'rb') as f: X_img_test = pickle.load(f)\n",
    "with open('content/movie_prediction/val_np_imgs_norm', 'rb') as f: X_img_val = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22179, 115, 75, 3)\n",
      "(6093, 115, 75, 3)\n",
      "(2452, 115, 75, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_img_train.shape)\n",
    "print(X_img_test.shape)\n",
    "print(X_img_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the overall datset and the train,test and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"content/movie_prediction/dataset_mod.csv\")\n",
    "train = pd.read_csv(\"content/movie_prediction/train_data.csv\")\n",
    "test = pd.read_csv(\"content/movie_prediction/test_data.csv\")\n",
    "val = pd.read_csv(\"content/movie_prediction/val_data.csv\")\n",
    "\n",
    "dataset['genre_list'] = dataset['genre_list'].apply(lambda x: ast.literal_eval(x))\n",
    "train['genre_list'] = train['genre_list'].apply(lambda x: ast.literal_eval(x))\n",
    "test['genre_list'] = test['genre_list'].apply(lambda x: ast.literal_eval(x))\n",
    "val['genre_list'] = val['genre_list'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "labels = {}\n",
    "\n",
    "for genre in test['genre_list']:\n",
    "    if len(genre) in labels:\n",
    "        labels[len(genre)] += 1\n",
    "    else:\n",
    "        labels[len(genre)] = 1\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(dataset['genre_list'].tolist())\n",
    "\n",
    "transformed_labels = mlb.fit_transform(dataset['genre_list'].tolist())\n",
    "\n",
    "train_labels = mlb.transform(train['genre_list'].tolist())\n",
    "\n",
    "test_labels = mlb.transform(test['genre_list'].tolist())\n",
    "\n",
    "val_labels = mlb.transform(val['genre_list'].tolist())\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.translate(str.maketrans('', '', punctuation))\n",
    "    text = text.lower().strip()\n",
    "    text = ' '.join([i if i not in stop and i.isalpha() else '' for i in text.lower().split()])\n",
    "    text = ' '.join([lemmatizer.lemmatize(w) for w in word_tokenize(text)])\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    return text\n",
    "\n",
    "train['overview'] = train['overview'].astype(str)\n",
    "test['overview'] = test['overview'].astype(str)\n",
    "val['overview'] = val['overview'].astype(str)\n",
    "\n",
    "train['overview'] = train['overview'].apply(lambda text: clean_text(text))\n",
    "test['overview'] = test['overview'].apply(lambda text: clean_text(text))\n",
    "val['overview'] = val['overview'].apply(lambda text: clean_text(text))\n",
    "\n",
    "dataset['overview'] = dataset['overview'].astype(str)\n",
    "dataset['overview'] = dataset['overview'].apply(lambda text: clean_text(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Embedding, LSTM, Dropout, Dense, Input, Bidirectional, Flatten, Conv2D, MaxPooling2D, concatenate, Conv1D, MaxPooling1D\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping\n",
    "\n",
    "MAX_NB_WORDS = 50000\n",
    "MAX_SEQUENCE_LENGTH = dataset['overview'].map(len).max()\n",
    "EMBEDDING_DIM = 300\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True)\n",
    "tokenizer.fit_on_texts(dataset['overview'].values)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_matrix(typeToLoad):\n",
    "    if typeToLoad == \"glove\":\n",
    "        EMBEDDING_FILE=\"/content/glove.twitter.27B.100d.txt\"\n",
    "        embed_size = 100\n",
    "    elif typeToLoad == \"word2vec\":\n",
    "        word2vecDict = KeyedVectors.load_word2vec_format(\"content/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "        embed_size = 300\n",
    "    elif typeToLoad == \"fasttext\":\n",
    "        EMBEDDING_FILE=\"/content/wiki-news-300d-1M.vec\"\n",
    "        embed_size = 300\n",
    "\n",
    "    if typeToLoad == \"glove\" or typeToLoad == \"fasttext\":\n",
    "        embeddings_index = dict()\n",
    "        f = open(EMBEDDING_FILE)\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        f.close()\n",
    "        print(\"Loaded \" + str(len(embeddings_index)) + \" word vectors.\")\n",
    "    else:\n",
    "        embeddings_index = dict()\n",
    "        for word in word2vecDict.wv.vocab:\n",
    "          embeddings_index[word] = word2vecDict.word_vec(word)\n",
    "        print(\"Loaded \" + str(len(embeddings_index)) + \" word vectors.\")\n",
    "        \n",
    "    embedding_matrix = 1 * np.random.randn(len(word_index)+1, embed_size)\n",
    "\n",
    "    embeddedCount = 0\n",
    "    for word, i in word_index.items():\n",
    "        i-=1\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            embeddedCount+=1\n",
    "    print(\"total embedded:\", embeddedCount, \"common words\")\n",
    "        \n",
    "    del(embeddings_index)\n",
    "        \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ming/anaconda3/envs/torch/lib/python3.7/site-packages/ipykernel_launcher.py:24: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3000000 word vectors.\n",
      "total embedded: 33902 common words\n"
     ]
    }
   ],
   "source": [
    "word2vec_embedding_matrix = get_embedding_matrix(\"word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69075, 300)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_train = tokenizer.texts_to_sequences(train['overview'].values)\n",
    "X_text_train = pad_sequences(X_text_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "X_text_test = tokenizer.texts_to_sequences(test['overview'].values)\n",
    "X_text_test = pad_sequences(X_text_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "X_text_val = tokenizer.texts_to_sequences(val['overview'].values)\n",
    "X_text_val = pad_sequences(X_text_val, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6093, 833)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347 347\n",
      "39 39\n",
      "96 96\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "X_img_val = np.reshape(X_img_val, (X_img_val.shape[0], 3, 75, 115))\n",
    "X_img_test = np.reshape(X_img_test, (X_img_test.shape[0], 3, 75, 115))\n",
    "X_img_train = np.reshape(X_img_train, (X_img_train.shape[0], 3, 75, 115))\n",
    "\n",
    "text_train_data = TensorDataset(torch.from_numpy(X_text_train), torch.from_numpy(train_labels))\n",
    "img_train_data = TensorDataset(torch.from_numpy(X_img_train), torch.from_numpy(train_labels))\n",
    "\n",
    "text_val_data = TensorDataset(torch.from_numpy(X_text_val), torch.from_numpy(val_labels))\n",
    "img_val_data = TensorDataset(torch.from_numpy(X_img_val), torch.from_numpy(val_labels))\n",
    "\n",
    "text_test_data = TensorDataset(torch.from_numpy(X_text_test), torch.from_numpy(test_labels))\n",
    "img_test_data = TensorDataset(torch.from_numpy(X_img_test), torch.from_numpy(test_labels))\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "text_train_loader = DataLoader(text_train_data, batch_size=batch_size)\n",
    "img_train_loader = DataLoader(img_train_data, batch_size=batch_size)\n",
    "\n",
    "text_val_loader = DataLoader(text_val_data, batch_size=batch_size)\n",
    "img_val_loader = DataLoader(img_val_data, batch_size=batch_size)\n",
    "\n",
    "text_test_loader = DataLoader(text_test_data, batch_size=batch_size)\n",
    "img_test_loader = DataLoader(img_test_data, batch_size=batch_size)\n",
    "\n",
    "print(len(text_train_loader), len(img_train_loader))\n",
    "print(len(text_val_loader), len(img_val_loader))\n",
    "print(len(text_test_loader), len(img_test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, weights_matrix, n_hidden, n_layers, n_out):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "\n",
    "        # LSTM for the text overview\n",
    "        self.vocab_size, self.n_hidden, self.n_out, self.n_layers = vocab_size, n_hidden, n_out, n_layers\n",
    "        num_embeddings, embedding_dim = weights_matrix.shape[0], weights_matrix.shape[1]\n",
    "        self.emb = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.emb.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
    "        self.emb.weight.requires_grad = True\n",
    "        self.lstm = nn.LSTM(embedding_dim, self.n_hidden, self.n_layers, dropout=0.2, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.lstm_fc = nn.Linear(self.n_hidden, 128)\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # CNN for the posters\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3)\n",
    "        self.max_pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.max_pool2 = nn.MaxPool2d(2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3)\n",
    "        self.max_pool3 = nn.MaxPool2d(2)\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3)\n",
    "        self.max_pool4 = nn.MaxPool2d(2)\n",
    "        self.cnn_dropout = nn.Dropout(0.1)\n",
    "        self.cnn_fc = nn.Linear(5*2*128, 512)\n",
    "\n",
    "        # Concat layer for the combined feature space\n",
    "        self.combined_fc1 = nn.Linear(640, 256)\n",
    "        self.combined_fc2 = nn.Linear(256, 128)\n",
    "        self.output_fc = nn.Linear(128, n_out)\n",
    "\n",
    "\n",
    "    def forward(self, lstm_inp, cnn_inp):\n",
    "        batch_size = lstm_inp.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        lstm_inp = lstm_inp.long()\n",
    "        embeds = self.emb(lstm_inp)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        lstm_out = self.dropout(lstm_out[:, -1])\n",
    "        lstm_out = F.relu(self.lstm_fc(lstm_out))\n",
    "\n",
    "        x = F.relu(self.conv1(cnn_inp))\n",
    "        x = self.max_pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.max_pool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.max_pool3(x)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.max_pool4(x)\n",
    "        x = x.view(-1, 5*2*128)\n",
    "        x = self.cnn_dropout(x)\n",
    "        cnn_out = F.relu(self.cnn_fc(x))\n",
    "\n",
    "        combined_inp = torch.cat((cnn_out, lstm_out), 1)\n",
    "        x_comb = F.relu(self.combined_fc1(combined_inp))\n",
    "        x_comb = F.relu(self.combined_fc2(x_comb))\n",
    "        out = torch.sigmoid(self.output_fc(x_comb))\n",
    "\n",
    "        return out\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
    "                          weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "CNN_LSTM(\n",
      "  (emb): Embedding(69075, 300)\n",
      "  (lstm): LSTM(300, 64, num_layers=2, batch_first=True, dropout=0.2)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (lstm_fc): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (max_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (max_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (max_pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (max_pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (cnn_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (cnn_fc): Linear(in_features=1280, out_features=512, bias=True)\n",
      "  (combined_fc1): Linear(in_features=640, out_features=256, bias=True)\n",
      "  (combined_fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (output_fc): Linear(in_features=128, out_features=18, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word_index)+1\n",
    "output_size = train_labels.shape[1]\n",
    "embedding_dim = 300\n",
    "hidden_dim = 64\n",
    "n_layers = 2\n",
    "print(output_size)\n",
    "\n",
    "model = CNN_LSTM(vocab_size, word2vec_embedding_matrix, hidden_dim, n_layers, output_size)\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "lr=0.001\n",
    "# criterion = nn.MultiLabelSoftMarginLoss()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "Epoch 1: train_loss: 0.2266 train_acc: 0.8632 | val_loss: 0.2749 val_acc: 0.8549\n",
      "Saving model...\n",
      "Epoch 2: train_loss: 0.2170 train_acc: 0.8690 | val_loss: 0.2743 val_acc: 0.8517\n",
      "Saving model...\n",
      "Epoch 3: train_loss: 0.2069 train_acc: 0.8755 | val_loss: 0.2803 val_acc: 0.8472\n",
      "Saving model...\n",
      "Epoch 4: train_loss: 0.1957 train_acc: 0.8825 | val_loss: 0.2837 val_acc: 0.8571\n",
      "Saving model...\n",
      "Epoch 5: train_loss: 0.1856 train_acc: 0.8886 | val_loss: 0.2862 val_acc: 0.8618\n",
      "Saving model...\n",
      "Epoch 6: train_loss: 0.1796 train_acc: 0.8925 | val_loss: 0.2851 val_acc: 0.8619\n",
      "Saving model...\n",
      "Epoch 7: train_loss: 0.1753 train_acc: 0.8954 | val_loss: 0.2853 val_acc: 0.8576\n",
      "Saving model...\n",
      "Epoch 8: train_loss: 0.1693 train_acc: 0.8989 | val_loss: 0.2934 val_acc: 0.8620\n",
      "Saving model...\n",
      "Epoch 9: train_loss: 0.1627 train_acc: 0.9031 | val_loss: 0.3205 val_acc: 0.8680\n",
      "Saving model...\n",
      "Epoch 10: train_loss: 0.1590 train_acc: 0.9055 | val_loss: 0.3087 val_acc: 0.8657\n",
      "Saving model...\n",
      "Epoch 11: train_loss: 0.1552 train_acc: 0.9078 | val_loss: 0.3146 val_acc: 0.8621\n",
      "Saving model...\n",
      "Epoch 12: train_loss: 0.1497 train_acc: 0.9111 | val_loss: 0.3244 val_acc: 0.8643\n",
      "Saving model...\n",
      "Epoch 13: train_loss: 0.1466 train_acc: 0.9130 | val_loss: 0.3262 val_acc: 0.8674\n",
      "Saving model...\n",
      "Epoch 14: train_loss: 0.1417 train_acc: 0.9155 | val_loss: 0.3251 val_acc: 0.8665\n",
      "Saving model...\n",
      "Epoch 15: train_loss: 0.1353 train_acc: 0.9194 | val_loss: 0.3405 val_acc: 0.8640\n",
      "Saving model...\n",
      "Epoch 16: train_loss: 0.1292 train_acc: 0.9229 | val_loss: 0.3483 val_acc: 0.8670\n",
      "Saving model...\n",
      "Epoch 17: train_loss: 0.1255 train_acc: 0.9250 | val_loss: 0.3758 val_acc: 0.8690\n",
      "Saving model...\n",
      "Epoch 18: train_loss: 0.1221 train_acc: 0.9273 | val_loss: 0.3680 val_acc: 0.8630\n",
      "Saving model...\n",
      "Epoch 19: train_loss: 0.1175 train_acc: 0.9300 | val_loss: 0.3863 val_acc: 0.8640\n",
      "Saving model...\n",
      "Epoch 20: train_loss: 0.1131 train_acc: 0.9325 | val_loss: 0.4183 val_acc: 0.8694\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "clip = 5\n",
    "\n",
    "model.train()\n",
    "\n",
    "for i in range(epochs):\n",
    "    total_acc_train = 0\n",
    "    total_loss_train = 0\n",
    "\n",
    "    for lstm, cnn in zip(text_train_loader, img_train_loader):\n",
    "        lstm_inp, lstm_labels = lstm\n",
    "        cnn_inp, cnn_labels = cnn\n",
    "        lstm_inp, lstm_labels = lstm_inp.to(device), lstm_labels.to(device)\n",
    "        cnn_inp, cnn_labels = cnn_inp.to(device), cnn_labels.to(device)\n",
    "        model.zero_grad()\n",
    "        output = model(lstm_inp, cnn_inp)\n",
    "        loss = criterion(output.squeeze(), lstm_labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            acc = torch.abs(output.squeeze() - lstm_labels.float()).view(-1)\n",
    "            acc = (1. - acc.sum() / acc.size()[0])\n",
    "            total_acc_train += acc\n",
    "            total_loss_train += loss.item()\n",
    "        \n",
    "\n",
    "    train_acc = total_acc_train/len(text_train_loader)\n",
    "    train_loss = total_loss_train/len(text_train_loader)\n",
    "    model.eval()\n",
    "    total_acc_val = 0\n",
    "    total_loss_val = 0\n",
    "    with torch.no_grad():\n",
    "        for lstm, cnn in zip(text_val_loader, img_val_loader):\n",
    "            lstm_inp, lstm_labels = lstm\n",
    "            cnn_inp, cnn_labels = cnn\n",
    "            lstm_inp, lstm_labels = lstm_inp.to(device), lstm_labels.to(device)\n",
    "            cnn_inp, cnn_labels = cnn_inp.to(device), cnn_labels.to(device)\n",
    "            model.zero_grad()\n",
    "            output = model(lstm_inp, cnn_inp)\n",
    "            val_loss = criterion(output.squeeze(), lstm_labels.float())\n",
    "            acc = torch.abs(output.squeeze() - lstm_labels.float()).view(-1)\n",
    "            acc = (1. - acc.sum() / acc.size()[0])\n",
    "            total_acc_val += acc\n",
    "            total_loss_val += val_loss.item()\n",
    "        print(\"Saving model...\") \n",
    "        torch.save(model.state_dict(), 'content/model/pytorch_word2vec_lstm_less_dropout.pt')\n",
    "    \n",
    "\n",
    "\n",
    "    val_acc = total_acc_val/len(text_val_loader)\n",
    "    val_loss = total_loss_val/len(text_val_loader)\n",
    "    print(f'Epoch {i+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
    "    model.train()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python(torch)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
